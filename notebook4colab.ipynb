{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for understanding music\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "\n",
    "\n",
    "# define the Pitch Dataset object for Pytorch\n",
    "class ImprovPitchDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    --> DataLoader can do the batch computation for us\n",
    "\n",
    "    Implement a custom Dataset:\n",
    "    inherit Dataset\n",
    "    implement __init__ , __getitem__ , and __len__\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        #for listing down the file names\n",
    "        import os\n",
    "        \n",
    "        #specify the path\n",
    "        path='data/w_jazz_augmented/'\n",
    "        #read all the filenames\n",
    "        files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "        #reading each midi file\n",
    "        notes_array = np.array([np.array(readMIDI(path+i)[0]) for i in files])\n",
    "        \n",
    "        #converting 2D array into 1D array\n",
    "        notes_ = [element for note_ in notes_array for element in note_]\n",
    "        #No. of unique notes\n",
    "        unique_notes = list(set(notes_))\n",
    "        print(\"number of unique pithces: \" + str(len(unique_notes)))\n",
    "        \n",
    "        from collections import Counter\n",
    "        #computing frequency of each note\n",
    "        freq = dict(Counter(notes_))\n",
    "              \n",
    "        # the threshold for frequent notes can change \n",
    "        threshold = 20 # this threshold is the number of classes that have to be predicted\n",
    "        frequent_notes = [note_ for note_, count in freq.items() if count>=threshold]\n",
    "        print(\"number of frequent pithces (more than 50 times): \" + str(len(frequent_notes)))\n",
    "        self.num_frequent_notes = len(frequent_notes)\n",
    "        self.vocab = frequent_notes\n",
    "        \n",
    "        # prepare new musical files which contain only the top frequent notes\n",
    "        new_music=[]\n",
    "        for notes in notes_array:\n",
    "            temp=[]\n",
    "            for note_ in notes:\n",
    "                if note_ in frequent_notes:\n",
    "                    temp.append(note_)            \n",
    "            new_music.append(temp)\n",
    "        new_music = np.array(new_music) # same solos but with only most frequent notes\n",
    "\n",
    "        self.x = new_music\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.x\n",
    "\n",
    "# define the Duration Dataset object for Pytorch\n",
    "class ImprovDurationDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    --> DataLoader can do the batch computation for us\n",
    "\n",
    "    Implement a custom Dataset:\n",
    "    inherit Dataset\n",
    "    implement __init__ , __getitem__ , and __len__\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        #for listing down the file names\n",
    "        import os\n",
    "        \n",
    "        #specify the path\n",
    "        path='data/w_jazz_augmented/'\n",
    "        #read all the filenames\n",
    "        files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "        #reading each midi file\n",
    "        notes_array = np.array([np.array(readMIDI(path+i)[1]) for i in files])\n",
    "        \n",
    "        #converting 2D array into 1D array\n",
    "        notes_ = [element for note_ in notes_array for element in note_]\n",
    "        #No. of unique notes\n",
    "        unique_notes = list(set(notes_))\n",
    "        print(\"number of unique durations: \" + str(len(unique_notes)))\n",
    "        \n",
    "        from collections import Counter\n",
    "        #computing frequency of each note\n",
    "        freq = dict(Counter(notes_))\n",
    "              \n",
    "        # the threshold for frequent notes can change \n",
    "        threshold = 10 \n",
    "        frequent_notes = [note_ for note_, count in freq.items() if count>=threshold]\n",
    "        print(\"number of frequent durations (more than 10 times): \" + str(len(frequent_notes)))\n",
    "        self.num_frequent_notes = len(frequent_notes)\n",
    "        self.vocab = frequent_notes\n",
    "        \n",
    "        # prepare new musical files which contain only the top frequent notes\n",
    "        new_music=[]\n",
    "        for notes in notes_array:\n",
    "            temp=[]\n",
    "            for note_ in notes:\n",
    "                if note_ in frequent_notes:\n",
    "                    temp.append(note_)            \n",
    "            new_music.append(temp)\n",
    "        new_music = np.array(new_music) # same solos but with only most frequent notes\n",
    "\n",
    "        self.x = new_music\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.x\n",
    "\n",
    "def readMIDI(file):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file : path to a midi file (string)\n",
    "        the midi file is read by the function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    notes : list of notes in midi number format\n",
    "        this list contains all of the notes and the rests in the song.\n",
    "        The duration of each note is in the correspondent index of the durations array\n",
    "    durations : list of durations in musical terms (string)\n",
    "        this list contains the durations of each note and rest in the song.\n",
    "        The pitch of each note is in the correspondent index of the durations array\n",
    "    dur_dict : python dictionary\n",
    "        the keys of this dictionary are the time in seconds \n",
    "        of each possible note/rest duration for this song.\n",
    "    song_properties : python dictionary\n",
    "        this dictionary contains some basic properties of the midi file.\n",
    "\n",
    "    '''\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI(file)\n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    # Get and downbeat times\n",
    "    beats = pm.get_beats()\n",
    "    downbeats = pm.get_downbeats()\n",
    "    \n",
    "    song_properties = {}\n",
    "    song_properties['tempo'] = pm.estimate_tempo()\n",
    "    song_properties['beat duration'] = beats[1] - beats[0]\n",
    "    song_properties['measure duration'] = downbeats[2] - downbeats[0]\n",
    "    \n",
    "    # sampling of the measure\n",
    "    unit = song_properties['measure duration'] / 96.\n",
    "    # possible note durations in seconds \n",
    "    # (it is possible to add representations - include 32nds, quintuplets...):\n",
    "    # [full, half, quarter, 8th, 16th, dot half, dot quarter, dot 8th, dot 16th, half note triplet, quarter note triplet, 8th note triplet]\n",
    "    possible_durations = [unit * 96, unit * 48, unit * 24, unit * 12, unit * 6, unit * 72, \n",
    "                          unit * 36, unit * 18, unit * 9, unit * 32, unit * 16, unit * 8]\n",
    "    \n",
    "    # Define durations dictionary\n",
    "    dur_dict = {}\n",
    "    dur_dict[possible_durations[0]] = 'full'\n",
    "    dur_dict[possible_durations[1]] = 'half'\n",
    "    dur_dict[possible_durations[2]] = 'quarter'\n",
    "    dur_dict[possible_durations[3]] = '8th'\n",
    "    dur_dict[possible_durations[4]] = '16th'\n",
    "    dur_dict[possible_durations[5]] = 'dot half'\n",
    "    dur_dict[possible_durations[6]] = 'dot quarter'\n",
    "    dur_dict[possible_durations[7]] = 'dot 8th'\n",
    "    dur_dict[possible_durations[8]] = 'dot 16th'\n",
    "    dur_dict[possible_durations[9]] = 'half note triplet'\n",
    "    dur_dict[possible_durations[10]] = 'quarter note triplet'\n",
    "    dur_dict[possible_durations[11]] = '8th note triplet'\n",
    "    \n",
    "    # compile the lists of pitchs and durations\n",
    "    notes = []\n",
    "    durations = []\n",
    "    for instrument in range(len(pm.instruments)):\n",
    "        for note in range(len(pm.instruments[instrument].notes)-1):\n",
    "            # append pitch\n",
    "            notes.append(str(pm.instruments[instrument].notes[note].pitch))\n",
    "            # calculate note duration in secods\n",
    "            duration_sec = pm.instruments[instrument].notes[note].end - pm.instruments[instrument].notes[note].start\n",
    "            # calculate distance from each duration\n",
    "            distance = np.abs(np.array(possible_durations) - duration_sec)\n",
    "            idx = distance.argmin()\n",
    "            durations.append(dur_dict[possible_durations[idx]])\n",
    "            \n",
    "            # check for rests\n",
    "            intra_note_time = pm.instruments[instrument].notes[note+1].start - pm.instruments[instrument].notes[note].end\n",
    "            # if the interval between notes is greater than the smallest duration ('16th')\n",
    "            # and smaller than the greatest duration ('full') then there is a rest\n",
    "            if intra_note_time >= possible_durations[4]:\n",
    "                # there is a rest!\n",
    "                \n",
    "                # handle the possibility of pauses longer than a full note\n",
    "                while intra_note_time > possible_durations[0]:\n",
    "                    notes.append('R')\n",
    "                    # calculate distance from each duration\n",
    "                    distance = np.abs(np.array(possible_durations) - intra_note_time)\n",
    "                    idx = distance.argmin()\n",
    "                    durations.append(dur_dict[possible_durations[idx]])\n",
    "                    intra_note_time -= possible_durations[idx]\n",
    "                \n",
    "                notes.append('R')\n",
    "                # calculate distance from each duration\n",
    "                distance = np.abs(np.array(possible_durations) - intra_note_time)\n",
    "                idx = distance.argmin()\n",
    "                durations.append(dur_dict[possible_durations[idx]])\n",
    "    return notes, durations, dur_dict, song_properties\n",
    "\n",
    "def getKey(val, dict_to_ix): \n",
    "    for key, value in dict_to_ix.items(): \n",
    "        if val == value: \n",
    "            return key\n",
    "\n",
    "def convertMIDI(notes, durations, tempo, dur_dict):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pitches : list of pitches\n",
    "        a list of all the pitches and the rests in the song to be exported.\n",
    "        Each pitch/rest should have its own duration \n",
    "        at the same index of the durations list.\n",
    "    durations : list of durations\n",
    "        a list of all the durations of the pitches/rests in the song to be exported.\n",
    "        Each duration should have its own pitch/rest \n",
    "        at the same index of the pitches list.\n",
    "    tempo : integer\n",
    "        tempo of the song.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pm : pretty_midi object\n",
    "        this pretty midi can be exported to midi and saved.\n",
    "\n",
    "    '''\n",
    "    # pitches and durations must be of equal lenght\n",
    "    # still does not include rests\n",
    "    \n",
    "    # Construct a PrettyMIDI object.\n",
    "    pm = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "    # Add a piano instrument\n",
    "    inst = pretty_midi.Instrument(program=1, is_drum=False, name='piano')\n",
    "    pm.instruments.append(inst)\n",
    "    # Let's add a few notes to our instrument\n",
    "    velocity = 100\n",
    "    offset = 0\n",
    "    # for each note\n",
    "    for i in range(len(notes)):\n",
    "        if notes[i] != '<pad>' and notes[i] != '<sos>' and notes[i] != '<eos>' and durations[i] != '<pad>' and durations[i] != '<sos>' and durations[i] != '<eos>':\n",
    "            if notes[i] == 'R':\n",
    "                duration = getKey(durations[i],dur_dict)\n",
    "            else:\n",
    "                pitch = int(notes[i])\n",
    "                duration = getKey(durations[i],dur_dict)\n",
    "                start = offset\n",
    "                end = offset + duration\n",
    "                inst.notes.append(pretty_midi.Note(velocity, pitch, start, end))\n",
    "            offset += duration\n",
    "    return pm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for understanding music\n",
    "import pretty_midi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataset_funct import ImprovDurationDataset, ImprovPitchDataset, readMIDI, convertMIDI\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# TRANSFORMER MODEL\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != src.size(0):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "# POSITIONAL ENCODING\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "import time\n",
    "def train(model, vocab, train_data, criterion, optimizer):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(vocab)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source, vocab):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(vocab)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # DATA LOADING\n",
    "    \n",
    "    # LOAD PITCH DATASET\n",
    "    datasetPitch = ImprovPitchDataset()\n",
    "    X_pitch = datasetPitch.getData()\n",
    "    # set vocabulary for conversion\n",
    "    vocabPitch = datasetPitch.vocab\n",
    "    # Add padding tokens to vocab\n",
    "    vocabPitch.append('<pad>')\n",
    "    vocabPitch.append('<sos>')\n",
    "    vocabPitch.append('<eos>')\n",
    "    pitch_to_ix = {word: i for i, word in enumerate(vocabPitch)}\n",
    "    #print(X_pitch[:3])\n",
    "    \n",
    "    # Divide pitch into train, validation and test\n",
    "    train_pitch = X_pitch[:int(len(X_pitch)*0.7)]\n",
    "    val_pitch = X_pitch[int(len(X_pitch)*0.7)+1:int(len(X_pitch)*0.7)+1+int(len(X_pitch)*0.1)]\n",
    "    test_pitch = X_pitch[int(len(X_pitch)*0.7)+1+int(len(X_pitch)*0.1):]\n",
    "    \n",
    "    # LOAD DURATION DATASET\n",
    "    datasetDuration = ImprovDurationDataset()\n",
    "    X_duration = datasetDuration.getData()\n",
    "    # set vocabulary for conversion\n",
    "    vocabDuration = datasetDuration.vocab\n",
    "    # Add padding tokens to vocab\n",
    "    vocabDuration.append('<pad>')\n",
    "    vocabDuration.append('<sos>')\n",
    "    vocabDuration.append('<eos>')\n",
    "    duration_to_ix = {word: i for i, word in enumerate(vocabDuration)}\n",
    "    #print(X_duration[:3])\n",
    "    \n",
    "    # Divide duration into train, validation and test\n",
    "    train_duration = X_duration[:int(len(X_duration)*0.7)]\n",
    "    val_duration = X_duration[int(len(X_duration)*0.7)+1:int(len(X_duration)*0.7)+1+int(len(X_duration)*0.1)]\n",
    "    test_duration = X_duration[int(len(X_duration)*0.7)+1+int(len(X_duration)*0.1):]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #%% DATA PREPARATION\n",
    "\n",
    "    # pad data to max_lenght of sequences, prepend <sos> and append <eos>\n",
    "    def pad(data):\n",
    "        # from: https://pytorch.org/text/_modules/torchtext/data/field.html\n",
    "        data = list(data)\n",
    "        # calculate max lenght\n",
    "        max_len = max(len(x) for x in data)\n",
    "        # Define padding tokens\n",
    "        pad_token = '<pad>'\n",
    "        init_token = '<sos>'\n",
    "        eos_token = '<eos>'\n",
    "        # pad each sequence in the data to max_lenght\n",
    "        padded, lengths = [], []\n",
    "        for x in data:\n",
    "            padded.append(\n",
    "                ([init_token])\n",
    "                + list(x[:max_len])\n",
    "                + ([eos_token])\n",
    "                + [pad_token] * max(0, max_len - len(x)))\n",
    "        lengths.append(len(padded[-1]) - max(0, max_len - len(x)))\n",
    "        return padded\n",
    "    \n",
    "    # divide into batches of size bsz and converts notes into numbers\n",
    "    def batchify(data, bsz, dict_to_ix):\n",
    "        \n",
    "        padded = pad(data)\n",
    "        padded_num = [[dict_to_ix[x] for x in ex] for ex in padded]\n",
    "        \n",
    "        data = torch.tensor(padded_num, dtype=torch.long)\n",
    "        data = data.contiguous()\n",
    "        \n",
    "        # Divide the dataset into bsz parts.\n",
    "        nbatch = data.size(0) // bsz\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        data = data.narrow(0, 0, nbatch * bsz)\n",
    "        # Evenly divide the data across the bsz batches.\n",
    "        data = data.view(bsz, -1).t().contiguous()\n",
    "        return data.to(device)\n",
    "    \n",
    "    batch_size = 20\n",
    "    eval_batch_size = 10\n",
    "    \n",
    "    train_data_pitch = batchify(train_pitch, batch_size, pitch_to_ix)\n",
    "    val_data_pitch = batchify(val_pitch, eval_batch_size, pitch_to_ix)\n",
    "    test_data_pitch = batchify(test_pitch, eval_batch_size, pitch_to_ix)\n",
    "    \n",
    "    train_data_duration = batchify(train_duration, batch_size, duration_to_ix)\n",
    "    val_data_duration = batchify(val_duration, eval_batch_size, duration_to_ix)\n",
    "    test_data_duration = batchify(test_duration, eval_batch_size, duration_to_ix)\n",
    "    \n",
    "    # divide into target and input sequence of lenght bptt\n",
    "    # --> obtain matrices of size bptt x batch_size\n",
    "    bptt = 35 # lenght of a sequence of data (IMPROVEMENT HERE!!)\n",
    "    def get_batch(source, i):\n",
    "        seq_len = min(bptt, len(source) - 1 - i)\n",
    "        data = source[i:i+seq_len] # input \n",
    "        target = source[i+1:i+1+seq_len].view(-1) # target (same as input but shifted by 1)\n",
    "        return data, target\n",
    "    \n",
    "    \n",
    "    #%% PITCH MODEL TRAINING\n",
    "    \n",
    "    # HYPERPARAMETERS\n",
    "    ntokens_pitch = len(vocabPitch) # the size of vocabulary\n",
    "    emsize = 200 # embedding dimension\n",
    "    nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead = 2 # the number of heads in the multiheadattention models\n",
    "    dropout = 0.2 # the dropout value\n",
    "    modelPitch = TransformerModel(ntokens_pitch, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0 # learning rate\n",
    "    optimizer = torch.optim.SGD(modelPitch.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    \n",
    "    # TRAIN AND EVALUATE LOSS\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs = 10 # The number of epochs\n",
    "    best_model = None\n",
    "    \n",
    "    # TRAINING LOOP\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(modelPitch, vocabPitch, train_data_pitch, criterion, optimizer)\n",
    "        val_loss = evaluate(modelPitch, val_data_pitch, vocabPitch)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "    \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_pitch = modelPitch\n",
    "    \n",
    "        scheduler.step()\n",
    "    \n",
    "    # TEST THE MODEL\n",
    "    test_loss = evaluate(best_model_pitch, test_data_pitch, vocabPitch)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "        test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n",
    "    \n",
    "    savePATHpitch = 'modelsPitch/modelPitch_'+ str(epochs) + 'epochs_padding.pt'\n",
    "    state_dictPitch = best_model_pitch.state_dict()\n",
    "    torch.save(state_dictPitch, savePATHpitch)\n",
    "    \n",
    "\n",
    "    #%% DURATION MODEL TRAINING\n",
    "    \n",
    "    # HYPERPARAMETERS\n",
    "    ntokens_duration = len(vocabDuration) # the size of vocabulary\n",
    "    emsize = 200 # embedding dimension\n",
    "    nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead = 2 # the number of heads in the multiheadattention models\n",
    "    dropout = 0.2 # the dropout value\n",
    "    modelDuration = TransformerModel(ntokens_duration, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0 # learning rate\n",
    "    optimizer = torch.optim.SGD(modelDuration.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    \n",
    "    # TRAIN AND EVALUATE LOSS\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs = 10 # The number of epochs\n",
    "    best_model = None\n",
    "    \n",
    "    # TRAINING LOOP\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(modelDuration, vocabDuration, train_data_duration, criterion, optimizer)\n",
    "        val_loss = evaluate(modelDuration, val_data_duration, vocabDuration)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "    \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_duration = modelDuration\n",
    "    \n",
    "        scheduler.step()\n",
    "    \n",
    "    # TEST THE MODEL\n",
    "    test_loss = evaluate(best_model_duration, test_data_duration, vocabDuration)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "        test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n",
    "    \n",
    "    savePATHduration = 'modelsDuration/modelDuration_'+ str(epochs) + 'epochs_padding.pt'\n",
    "    state_dictDuration = best_model_duration.state_dict()\n",
    "    torch.save(state_dictDuration, savePATHduration)\n",
    "    \n",
    "\n",
    "    #%% SAMPLES GENERATION\n",
    "\n",
    "    def getNote(val, dict_to_ix): \n",
    "        for key, value in dict_to_ix.items(): \n",
    "             if val == value: \n",
    "                 return key\n",
    "\n",
    "    def generate(model, melody4gen, dict_to_ix, next_notes=10):\n",
    "        melody4gen = melody4gen.tolist()\n",
    "        for i in range(0,next_notes):\n",
    "            x_pred = torch.tensor([dict_to_ix[w] for w in melody4gen], dtype=torch.long)\n",
    "            y_pred = model(x_pred)\n",
    "            last_word_logits = y_pred[0][-1]\n",
    "            p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "            melody4gen.append(getNote(word_index, dict_to_ix))\n",
    "        return melody4gen\n",
    "    \n",
    "    # Remove characters who are not in the dictionary\n",
    "    def onlyDict(pitchs, durations, vocabPitch, vocabDuration):\n",
    "        # takes an array and a dictionary and gives the same array without\n",
    "        # the elements who are not in the dictionary\n",
    "        new_pitch = []\n",
    "        new_duration = []\n",
    "        for i in range(len(pitchs)):\n",
    "            if pitchs[i] in vocabPitch and durations[i] in vocabDuration:\n",
    "                new_pitch.append(pitchs[i]) \n",
    "                new_duration.append(durations[i]) \n",
    "        new_pitch = np.array(new_pitch) # same solos but with only most frequent notes\n",
    "        new_duration = np.array(new_duration) # same solos but with only most frequent notes\n",
    "        return new_pitch, new_duration\n",
    "    \n",
    "    \n",
    "    #specify the path\n",
    "    f = 'data/w_jazz/JohnColtrane_Mr.P.C._FINAL.mid'\n",
    "    melody4gen_pitch, melody4gen_duration, dur_dict, song_properties = readMIDI(f)\n",
    "    melody4gen_pitch, melody4gen_duration = onlyDict(melody4gen_pitch, melody4gen_duration, vocabPitch, vocabDuration)\n",
    "    melody4gen_pitch = melody4gen_pitch[:80]\n",
    "    melody4gen_duration = melody4gen_duration[:80]\n",
    "    #print(melody4gen_pitch)\n",
    "    #print(melody4gen_duration)\n",
    "    \n",
    "    notes2gen = 40 # number of new notes to generate\n",
    "    new_melody_pitch = generate(modelPitch, melody4gen_pitch, pitch_to_ix, next_notes=notes2gen)\n",
    "    new_melody_duration = generate(modelDuration, melody4gen_duration, duration_to_ix, notes2gen)\n",
    "\n",
    "    converted = convertMIDI(new_melody_pitch, new_melody_duration, song_properties['tempo'], dur_dict)\n",
    "    converted.write('output/music.mid')\n",
    "    \n",
    "    # For plotting\n",
    "    import mir_eval.display\n",
    "    import librosa.display\n",
    "    import matplotlib.pyplot as plt\n",
    "    def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
    "        # Use librosa's specshow function for displaying the piano roll\n",
    "        librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
    "                                 hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
    "                                 fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot_piano_roll(converted, 0, 127)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
